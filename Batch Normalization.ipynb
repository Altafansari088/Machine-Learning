{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7110b7b4-7319-48b0-ad84-ea37064ed6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Objective: The objective of this assignment is to assess students' understanding of batch normalization in\n",
    "artificial neural networks (ANN) and its impact on training performance.\n",
    "Q1. Theory and ConceptsU\n",
    "Sr Explain the concept of batch normalization in the context of Artificial Neural Networksr\n",
    "Er Describe the benefits of using batch normalization during trainingr\n",
    "@r Discuss the working principle of batch normalization, including the normalization step and the learnable\n",
    "parameters.\n",
    "Q2. ImplementationU\n",
    "Sr Choose a dataset of your choice (e.g., MNIST, CIFAR-10) and preprocess itr\n",
    "Er Implement a simple feedforward neural network using any deep learning framework/library (e.g.,\n",
    "TensorFlow, PyTorch)r\n",
    "@r Train the neural network on the chosen dataset without using batch normalizationr\n",
    "Âžr Implement batch normalization layers in the neural network and train the model againr\n",
    "ur Compare the training and validation performance (e.g., accuracy, loss) between the models with and\n",
    "without batch normalizationr\n",
    "tr Discuss the impact of batch normalization on the training process and the performance of the neural\n",
    "network.\n",
    "Q3. Experimentation and AnalysisU\n",
    "Sr Experiment with different batch sizes and observe the effect on the training dynamics and model\n",
    "performancer\n",
    "Er Discuss the advantages and potential limitations of batch normalization in improving the training of\n",
    "neural networks.\n",
    "Submission GuidelinesU\n",
    "/ Complete the assignment in a Jupyter Notebook\n",
    "/ Include necessary comments and explanations to make your code understandable\n",
    "/ Provide visualizations, tables, and explanations for your analysis and findings\n",
    "/ Create a GitHub repository to host your assignment files\n",
    "/ Rename your Jupyter Notebook file using the format \"date_month_topic.ipynb\" (e.g.,\n",
    "\"12_July_Regression.ipynb\")\n",
    "/ Place your Jupyter Notebook file (.ipynb) in the repository\n",
    "/ Ensure that the notebook runs without errors\n",
    "/ Commit and push any additional files or resources required to run your code (if applicable) to the\n",
    "repository\n",
    "/ Make sure the repository is publicly accessible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d3c0c-f441-4260-a141-6cb5ae1987d0",
   "metadata": {},
   "source": [
    "o compare the performance of models with and without batch normalization, you can train two neural networks with the same architecture and hyperparameters, but one with batch normalization layers and the other without. Then, evaluate the performance metrics (e.g., accuracy, loss) of both models on a validation or test dataset.\n",
    "To discuss the impact of batch normalization on the training process and performance of the neural network, you can analyze the following aspects:\n",
    "Convergence speed: Compare the training curves of both models and observe how quickly they converge. Batch normalization can help accelerate convergence by reducing the internal covariate shift and stabilizing the gradients.\n",
    "\n",
    "Generalization performance: Evaluate the performance of both models on unseen data. Batch normalization can improve generalization by reducing overfitting and allowing the model to better adapt to different instances in the dataset.\n",
    "Robustness to hyperparameters: Experiment with different learning rates, batch sizes, and regularization techniques for both models. Analyze how batch normalization affects the sensitivity of the model to these hyperparameters and its ability to find a good solution.\n",
    "Training dynamics: Analyze the behavior of the gradients and activations during training with and without batch normalization. Batch normalization can help mitigate the vanishing/exploding gradient problem and ensure more stable and consistent training dynamics.\n",
    "To experiment with different batch sizes and observe the effect on the training dynamics and performance, you can train the model with various batch sizes and analyze the following:\n",
    "\n",
    "Convergence speed: Compare the training curves and observe how the batch size affects the convergence speed. Smaller batch sizes may lead to faster convergence but with more noisy updates, while larger batch sizes may lead to slower convergence but with more stable updates.\n",
    "Generalization performance: Evaluate the performance of the model with different batch sizes on a validation or test dataset. Analyze how the batch size affects the model's ability to generalize to unseen data.\n",
    "\n",
    "Computational efficiency: Measure the training time for different batch sizes. Smaller batch sizes may require more iterations to converge but can be more computationally expensive due to the overhead of updating the model parameters more frequently.\n",
    "Stability and robustness: Observe how the batch size affects the stability of the training process. Smaller batch sizes may introduce more randomness and make the training process less stable, while larger batch sizes may lead to more stable updates but with a higher risk of getting stuck in local minima.\n",
    "Remember to include necessary code, comments, and explanations in your Jupyter Notebook to support your analysis and findings. Ensure that your notebook runs without errors and is well-organized. Commit and push any additional files or resources required to run your code to the repository, and make sure the repository is publicly accessible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
