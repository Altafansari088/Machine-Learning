{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e71a06-7ad8-44dd-8998-01388ff74060",
   "metadata": {},
   "source": [
    "Q1. How does bagging reduce overfitting in decision trees?\n",
    "\n",
    "Bagging reduces overfitting in decision trees by creating multiple subsets of the original dataset through bootstrapping. Each subset is used to train a separate decision tree model. By averaging the predictions of these individual trees, bagging reduces the variance and helps to generalize the model. This ensemble approach helps to reduce the impact of individual noisy or outlier data points, leading to a more robust and less overfit model.\n",
    "\n",
    "Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n",
    "\n",
    "A2. The advantages of using different types of base learners in bagging include increased diversity in the ensemble, which can improve the overall performance and robustness of the model. Different base learners may have different strengths and weaknesses, allowing them to capture different aspects of the data. However, the disadvantage is that using complex base learners may increase the computational complexity and training time of the ensemble.\n",
    "\n",
    "Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
    "\n",
    " The choice of base learner in bagging can affect the bias-variance tradeoff. If the base learner has high bias, such as a decision stump, the ensemble may have high bias as well. On the other hand, if the base learner has high variance, such as a deep decision tree, the ensemble may have high variance too. Therefore, the choice of base learner should be made carefully to strike a balance between bias and variance in order to achieve optimal performance.\n",
    "\n",
    "Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
    "\n",
    " Yes, bagging can be used for both classification and regression tasks. In classification, bagging combines the predictions of multiple base classifiers (e.g., decision trees) to make a final prediction. The class with the majority vote is typically chosen as the final prediction. In regression, bagging combines the predictions of multiple base regression models to obtain an average or a weighted average prediction. The final prediction is the mean or median of the individual predictions.\n",
    "Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n",
    "\n",
    " The ensemble size, or the number of models included in bagging, plays a role in the performance of the ensemble. Increasing the ensemble size generally improves the performance up to a certain point, after which the improvement becomes marginal. Adding more models beyond this point may not significantly improve the performance but will increase the computational cost. The optimal ensemble size depends on the specific problem and dataset, and it is often determined through experimentation and cross-validation.\n",
    "\n",
    "Q6. Can you provide an example of a real-world application of bagging in machine learning?\n",
    "\n",
    " A real-world application of bagging in machine learning is in the field of medical diagnosis. Bagging can be used to create an ensemble of decision trees trained on different subsets of patient data. Each decision tree can make predictions about a patient's diagnosis based on their symptoms, medical history, and test results. By combining the predictions of multiple decision trees, bagging can provide a more accurate and reliable diagnosis, reducing the risk of misdiagnosis and improving patient care."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
