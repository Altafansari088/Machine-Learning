{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7756b5a5-10ba-4ae8-ae47-7bd35c9b7a41",
   "metadata": {},
   "source": [
    "Q1. What is KNN classifier and KNN regressor?\n",
    "\n",
    "KNN (K-Nearest Neighbors) is a supervised machine learning algorithm that can be used for both classification and regression tasks.\n",
    "KNN Classifier: In KNN classification, the algorithm predicts the class label of a new data point based on the majority class labels of its k nearest neighbors in the training data. The class label is assigned to the new data point based on a majority vote among its neighbors.\n",
    "KNN Regressor: In KNN regression, the algorithm predicts the continuous value of a new data point based on the average or weighted average of the target values of its k nearest neighbors in the training data. The predicted value is calculated as the mean or weighted mean of the target values of the neighbors.\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "The choice of the value of K in KNN is an important consideration as it can significantly impact the performance of the algorithm. The selection of K depends on the characteristics of the dataset and the problem at hand. Here are a few approaches to choose the value of K:\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "The main difference between KNN classifier and KNN regressor lies in the type of output they produce.\n",
    "KNN Classifier: In KNN classification, the algorithm predicts the class label of a new data point based on the class labels of its k nearest neighbors in the training data. The predicted class label is determined by majority voting, where the class label that occurs most frequently among the neighbors is assigned to the new data point.\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "The performance of a KNN algorithm can be measured using various evaluation metrics, depending on whether it is a classification or regression task.\n",
    "For KNN Classification:\n",
    "Accuracy: It measures the proportion of correctly classified instances out of the total instances.\n",
    "Precision: It measures the proportion of true positive predictions out of the total positive predictions.\n",
    "Recall: It measures the proportion of true positive predictions out of the total actual positive instances.\n",
    "F1 Score: It is the harmonic mean of precision and recall, providing a balanced measure of both metrics.\n",
    "Confusion Matrix: It provides a detailed breakdown of the true positive, true negative, false positive, and false negative predictions.\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "The curse of dimensionality refers to the phenomenon where the performance of KNN and other distance-based algorithms deteriorates as the number of features or dimensions increases. In high-dimensional spaces, the data becomes sparse, and the notion of distance becomes less meaningful.\n",
    "The curse of dimensionality can lead to several challenges in KNN:\n",
    "Increased computational complexity: As the number of dimensions increases, the number of distances that need to be calculated grows exponentially, making the algorithm computationally expensive.\n",
    "Increased data sparsity: In high-dimensional spaces, the data points become more spread out, making it difficult to find meaningful nearest neighbors.\n",
    "Increased risk of overfitting: With a large number of dimensions, the algorithm may become more sensitive to noise and outliers, leading to overfitting.\n",
    "To mitigate the curse of dimensionality in KNN, dimensionality reduction techniques such as feature selection or feature extraction can be applied to reduce the number of irrelevant or redundant features. Additionally, using distance metrics that are robust to high-dimensional spaces, such as cosine similarity or Mahalanobis distance, can help improve the performance of KNN.\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Handling missing values in KNN involves imputing or filling in the missing values before applying the algorithm. Here are some common approaches to handle missing values in KNN:\n",
    "Deletion: If the dataset has a small number of missing values, the simplest approach is to delete the instances or features with missing values. However, this approach may lead to loss of valuable information if the missing values are not randomly distributed.\n",
    "Mean/Median/Mode imputation: Missing values can be replaced with the mean, median, or mode of the available values in the same feature. This approach assumes that the missing values are missing at random and that the distribution of the available values is representative of the missing values.\n",
    "KNN imputation: In this approach, the missing values are imputed based on the values of their k nearest neighbors. The algorithm calculates the distances between the instances and uses the values of the nearest neighbors to estimate the missing values. The average or weighted average of the neighbors' values can be used for imputation.\n",
    "Regression imputation: If the feature with missing values is continuous, regression models can be used to predict the missing values based on the other features. The regression model is trained on the instances with complete data and used to estimate the missing values.\n",
    "The choice of the imputation method depends on the nature of the missing data and the specific dataset. It is important to consider the potential impact of imputation on the performance and validity of the KNN algorithm.\n",
    "Q7. Compare and contrast the performance of the KNN classifier and KNN regressor.\n",
    "\n",
    "KNN Classifier and KNN Regressor have some similarities and differences in terms of their performance:\n",
    "Similarities:\n",
    "\n",
    "Both algorithms are based on the K-nearest neighbors approach, where the prediction is made based on the majority vote or average of the nearest neighbors.\n",
    "\n",
    "Both algorithms are non-parametric, meaning they do not make assumptions about the underlying data distribution.\n",
    "\n",
    "Both algorithms can handle multi-class classification problems, although KNN Regressor is primarily used for regression tasks.\n",
    "\n",
    "Differences:\n",
    "\n",
    "KNN Classifier predicts discrete class labels, while KNN Regressor predicts continuous target values.\n",
    "\n",
    "The evaluation metrics used to measure their performance differ. KNN Classifier is typically evaluated using metrics such as accuracy, precision, recall, and F1 score, while KNN Regressor is evaluated using metrics such as mean squared error, root mean squared error, mean absolute error, and R-squared.\n",
    "\n",
    "The choice of k (the number of nearest neighbors) may differ between the two algorithms. In KNN Classifier, a smaller k value may be preferred to avoid ties in the majority vote, while in KNN Regressor, a larger k value may be used to reduce the impact of outliers.\n",
    "\n",
    "The decision boundaries of KNN Classifier are often nonlinear and can be complex, while KNN Regressor produces a smooth regression surface.\n",
    "\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "The KNN algorithm has several strengths and weaknesses for both classification and regression tasks:\n",
    "Strengths of KNN:\n",
    "Simple and easy to understand: KNN is a straightforward algorithm that is easy to implement and interpret.\n",
    "Non-parametric: KNN does not make any assumptions about the underlying data distribution, making it suitable for a wide range of problems.\n",
    "Flexibility: KNN can handle both classification and regression tasks.\n",
    "Ability to capture complex relationships: KNN can capture complex nonlinear relationships between features and the target variable.\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN:\n",
    "Euclidean distance: Euclidean distance is the straight-line distance between two points in Euclidean space. It is calculated as the square root of the sum of squared differences between corresponding coordinates. In other words, it measures the length of the shortest path between two points. Euclidean distance is suitable for continuous data and when the relationship between features is linear.\n",
    "Manhattan distance: Manhattan distance, also known as city block distance or L1 distance, is the sum of absolute differences between corresponding coordinates. It measures the distance between two points by summing the absolute differences along each dimension. Manhattan distance is suitable for categorical or ordinal data and when the relationship between features is non-linear.\n",
    "\n",
    "Q10. How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "\n",
    "The AdaBoost algorithm updates the weights of misclassified samples in each iteration to give more importance to the misclassified samples and focus on them in subsequent iterations. The weight update process in AdaBoost can be summarized as follows:\n",
    "Initialize the sample weights: At the beginning of the algorithm, all samples are assigned equal weights.\n",
    "Train a weak learner: A weak learner, such as a decision tree or a shallow neural network, is trained on the training data considering the sample weights. The weak learner aims to minimize the weighted error rate.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
