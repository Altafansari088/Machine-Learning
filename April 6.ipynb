{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2915a64-3820-4101-9b0d-ab7a48035ac5",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "The mathematical formula for a linear SVM can be represented as:\n",
    "f(x) = sign(w^T * x + b)\n",
    "where:\n",
    "f(x) is the predicted class label for input x\n",
    "w is the weight vector\n",
    "b is the bias term\n",
    "x is the input vector\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "The objective function of a linear SVM is to find the hyperplane that maximizes the margin between the two classes while minimizing the classification error. Mathematically, it can be represented as:\n",
    "\n",
    "minimize: 1/2 * ||w||^2 + C * Î£(max(0, 1 - y_i * (w^T * x_i + b)))\n",
    "where:\n",
    "\n",
    "||w||^2 is the regularization term that controls the complexity of the model\n",
    "C is the regularization parameter that balances the trade-off between maximizing the margin and minimizing the classification error\n",
    "y_i is the true class label of the i-th training example\n",
    "x_i is the i-th training example\n",
    "w and b are the parameters to be learned\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "The kernel trick in SVM is a technique that allows SVMs to efficiently handle non-linearly separable data by implicitly mapping the input data into a higher-dimensional feature space. Instead of explicitly computing the mapping, the kernel function calculates the dot product between the mapped feature vectors, avoiding the need to compute the actual mapping. This allows SVMs to effectively learn non-linear decision boundaries without explicitly transforming the data.\n",
    "\n",
    "Q4. What is the role of support vectors in SVM? Explain with an example.\n",
    "\n",
    "Support vectors are the data points that lie closest to the decision boundary (hyperplane) in SVM. They play a crucial role in defining the decision boundary and determining the classification of new data points. The support vectors are the critical points that influence the position and orientation of the decision boundary.\n",
    "For example, consider a binary classification problem with two classes, represented by red and blue points. The decision boundary is a line that separates the two classes. The support vectors are the points that lie on or closest to the decision boundary. These points determine the position and orientation of the decision boundary and are essential for making predictions on new data points.\n",
    "\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM.\n",
    "\n",
    "Unfortunately, as a text-based AI model, I am unable to provide visual illustrations or graphs directly. However, I can describe these concepts briefly:\n",
    "Hyperplane: In SVM, the hyperplane is the decision boundary that separates the data points of different classes. In a linear SVM, the hyperplane is a line in two dimensions or a plane in higher dimensions.\n",
    "\n",
    "Marginal plane: The marginal plane in SVM refers to the planes parallel to the hyperplane that touch the support vectors. These planes define the margin, which is the distance between the hyperplane and the support vectors.\n",
    "\n",
    "Soft margin: Soft margin SVM allows for some misclassification of training examples by introducing a slack variable. It allows for a more flexible decision boundary that can handle some overlapping or noisy data points.\n",
    "\n",
    "Q6. SVM Implementation through the Iris dataset.\n",
    "\n",
    "To implement SVM on the Iris dataset, you can follow these steps:\n",
    "\n",
    "Load the Iris dataset from the scikit-learn library using the load_iris() function.\n",
    "\n",
    "Split the dataset into a training set and a testing set using the train_test_split() function from scikit-learn.\n",
    "\n",
    "Train a linear SVM classifier on the training set using the LinearSVC class from scikit-learn.\n",
    "\n",
    "Predict the labels for the testing set using the trained model's predict() function.\n",
    "\n",
    "Compute the accuracy of the model on the testing set by comparing the predicted labels with the true labels.\n",
    "\n",
    "Plot the decision boundaries of the trained model using two of the features. You can use a scatter plot to visualize the data points and the decision boundaries.\n",
    "Experiment with different values of the regularization parameter C and observe how it affects the performance of the model. You can train and evaluate the model for different values of C and compare the accuracy or other performance metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
